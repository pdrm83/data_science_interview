{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textrank_summarization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-UXEXV-ifo6",
        "colab_type": "text"
      },
      "source": [
        "### Read input data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAxFJY6xlI0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vt_4jPklI4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myOIMc87lRuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the raw data file\n",
        "downloaded = drive.CreateFile({'id':\"154ZPP7J54KPmq8TuFfX8kgAHb2ah0neo\"})\n",
        "downloaded.GetContentFile('design_thinking_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xztilg9zlRxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read file as panda dataframe\n",
        "import pandas as pd\n",
        "\n",
        "raw_data = pd.read_csv('design_thinking_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSSPld8jnqp2",
        "colab_type": "text"
      },
      "source": [
        "### Clean up text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEP-GX0tnpp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split lines by '\\n' character\n",
        "import os \n",
        "\n",
        "LINE_SEP = os.linesep\n",
        "raw_data['clean_text'] = raw_data['article_text'].apply(lambda s: str(s).split(LINE_SEP))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dpfh4MAnpxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a5aae2c9-20ea-4862-b398-aaf1cb4aeef7"
      },
      "source": [
        "# download NLTK punctuations & stop words\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M8GcGk9npt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the the text in the articles into sentences\n",
        "from nltk.tokenize import sent_tokenize \n",
        "\n",
        "sentences = []\n",
        "for article in raw_data['clean_text']:\n",
        "    for paragraph in article:\n",
        "        sentences.append(sent_tokenize(paragraph)) if paragraph else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwffxhL_np0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten the list\n",
        "sentences = [y for x in sentences for y in x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pADAtHvenp3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove punctuations, numbers and special characters\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "# make alphabets lowercase\n",
        "clean_sentences = [s.lower() for s in clean_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQniQHZDnp66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove stop words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XqpKMsVnp-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop empty items in the list\n",
        "clean_sentences = [clean_sentence for clean_sentence in clean_sentences if clean_sentence != '']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s3ODFwFszXD",
        "colab_type": "text"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LljC5Vs6q7as",
        "colab_type": "text"
      },
      "source": [
        "#### Word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jngGGEw1nqHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the Glove data file\n",
        "downloaded = drive.CreateFile({'id':\"1XlK7waXNOsGf3mdgMWiLVrDhCgmnDhr1\"})\n",
        "downloaded.GetContentFile('glove.6B.100d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcs9efyviN8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract word vectors from GloVe\n",
        "from numpy import asarray\n",
        "\n",
        "word_embeddings = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = asarray(values[1:], dtype='float32')\n",
        "        word_embeddings[word] = coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyiT5sFftBUM",
        "colab_type": "text"
      },
      "source": [
        "#### Sentence vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vl30p4MiN-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build sentence vectors\n",
        "from numpy import zeros\n",
        "\n",
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "    if len(i) != 0:\n",
        "        v = sum([word_embeddings.get(w, zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "    else:\n",
        "        v = zeros((100,))\n",
        "    sentence_vectors.append(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY9toPVXq_FN",
        "colab_type": "text"
      },
      "source": [
        "### TextRank algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXsRqz2OrCde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aac86e33-4d53-4806-e372-d2f6ef6ca1d2"
      },
      "source": [
        "# similarity matrix\n",
        "import time\n",
        "\n",
        "from scipy import sparse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sentence_vectors_sparse = sparse.csr_matrix(sentence_vectors)\n",
        "\n",
        "start_time = time.time()\n",
        "similarities = cosine_similarity(sentence_vectors_sparse)\n",
        "print(f\"Similarity matrix calculated in {time.time() - start_time:.2f} seconds\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity matrix calculated in 0.46 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCpiQN0frCfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(similarities)\n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2tkoYuyDrFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ranks\n",
        "ranked_sentences = []\n",
        "\n",
        "for i, s in enumerate(sentences[:100]): \n",
        "  ranked_sentences.append((scores[i], s))\n",
        "\n",
        "ranked_sentences.sort(key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtHTsvDEFtxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjuBU2K3rCsD",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH4n4xGAvujM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "177a6779-25eb-4bdb-c92d-1bfb67e64601"
      },
      "source": [
        "# Specify number of sentences to form the summary\n",
        "sn = 15\n",
        "\n",
        "# Generate summary\n",
        "for i in range(sn):\n",
        "        print(f\"\\n{ranked_sentences[i][1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "are developed.\n",
            "\n",
            "[9] Whereas for \"tame\" or \"well-defined\" problems the problem is clear, and the solution is available through applying rules or technical knowledge.\n",
            "\n",
            "[8]\n",
            "\n",
            "[7] Core features of design thinking include the abilities to:\n",
            "\n",
            "[5][6]\n",
            "\n",
            "[3][4] Some of these prescriptions have been criticized for oversimplifying the design process and trivializing the role of technical knowledge and skills.\n",
            "\n",
            "[28][4] In the 2000s there was a significant growth of interest in design thinking as a catalyst for gaining competitive advantage within business,[29] but doubts around design thinking as a panacea for success have also been expressed.\n",
            "\n",
            "[26][27] Designers approach users with the goal of understanding their wants and needs, what might make their life easier and more enjoyable and how technology can be useful for them.\n",
            "\n",
            "[25]\n",
            "\n",
            "[25]\n",
            "\n",
            "[25]\n",
            "\n",
            "[25]\n",
            "\n",
            "[24] Projects may loop back through inspiration, ideation, and implementation more than once as the team refines its ideas and explores new directions.\n",
            "\n",
            "[23] Plattner, Meinel, and Leifer state: \"While the stages are simple enough, the adaptive expertise required to choose the right inflection points and appropriate next stage is a high order intellectual activity that requires practice and is learnable.\"\n",
            "\n",
            "[21][22]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPO93ux_rIqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}